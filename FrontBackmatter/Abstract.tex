%*******************************************************
% Abstract
%*******************************************************
%\renewcommand{\abstractname}{Abstract}
\pdfbookmark[1]{Abstract}{Abstract}
\begingroup
\let\clearpage\relax
\let\cleardoublepage\relax
\let\cleardoublepage\relax

\chapter*{Abstract}

Visual localisation and object recognition are key goals of artificial intelligence research that have been traditionally investigated separately. Appearance-based methods can be used to treat both problems from the same perspective. Therefore, the main purpose of this thesis is to explore appearance-based methods in the specific contexts of wearable and hand-held object recognition and visual localisation. Specifically, the contributions of this thesis is as follows:

In first place, we have collected two large datasets and studied the constraints of the data problem in the cases of object recognition and indoor visual localisation when the visual input (images or video) are acquired by wearable or hand-held devices. In second place, using these datasets and those provided by the community, we constructed evaluation pipelines for object recognition and visual localisation; and benchmarked custom-created image desription methods with baseline and state-of-the-art ones for performance in this context. In third, place, we have developed a model of the biologicall place cells, the artificial place cells based on kernel distance metrics of appearance-based methods between query and database images. We also tested their performance under the challenging conditions of indoor localisation, resulting in a successful compared with the state of the art SLAM. Finally, we have prototyped an assistive localisation system using wearable or hand-held visual input and tactile feedback to track the localisation of the user over haptic maps.


\vfill

\pdfbookmark[1]{Resumen}{Resumen}
\chapter*{Resumen}
Abstract in Spanish\dots


\endgroup			

\vfill