%************************************************
\chapter{Appearance-based indoor localisation: A comparison of patch descriptor performance}\label{ch:chapter4} 
%************************************************

\section{Introduction}
\label{sec:intro}

Self-localisation within indoor spaces has numerous real-world applications, ranging from navigation inside public spaces and large shopping and social environments to assistive devices for people with visual impairment.   Harvesting information from radio-strength signals and radio beacons to perform localisation is an emerging technology~\cite{wang2012no,fallah2013indoor}.  However, few potential solutions are as compelling as those using  visual information,  captured from wearable or hand-held cameras, and conveyed into knowledge about how to navigate a space. 

This work proposes an alternative approach to geometric and SLAM-based localisation.  Location is, instead, inferred through visual queries against the journeys of other users, rather than by explicit map-building or geometric inference. 
I test this idea  in a new dataset of \emph{visual paths}~\cite{Rivera-Rubio2014}, containing  more than \SI{3}{km} of video sequences captured through {\em multiple} passes along 10 corridors  in a large building with ground truth. I compare custom-designed descriptors  with SIFT~\cite{Lowe2004}, HOG3D~\cite{Klaser2008} and a state-of-the-art SLAM method: large-scale direct monocular SLAM (LSD-SLAM).  Standard bag-of-visual-words (BoVWs) a\-pproach\-es are used to index and associate views between journeys. The results suggest that, even without tracking, significant cues can be captured and used to infer location.  The application to wearable camera technology -- whereby image cues are harvested from volunteered journeys, can then be used to help other users of the same space navigate -- is the eventual goal of this work, which is a natural extension to recently reported approaches based on harvesting environmental signals from smartphones \cite{Wang2012}.


%We do not, for one moment, suggest that one would attempt to use vision to localise without incorporating data from other sensors that are worn, including radio signal strength, accelerometers and magnetometers.  Instead, 

%------------------------------------------------------------------------- 
\section{Related work}
\label{sec:retrieval}

\subsection{Matching between visual paths} 
\label{subsec:early_works}
 
As I briefly introduced in Chapter \ref{ch:chapter2}, a \textit{visual path} can be seen as a collection of image frames that are induced by the relative motion of a person in a scene. The work reported in this chapter involves matching the visual paths of a ``new'' journey instance to previous, similar instances. 

Early work by Matsumoto et al. \cite{Matsumoto1996} introduced a similar concept of the ``view-sequenced route representation''.  In this scheme,  a robot could perform simple navigation tasks by correlating current views against those held in a database. \citet{Ohno1996} also worked on this idea, using the difference between frames of detected vertical lines to estimate changes in position and orientation. Their results were constrained to controlled robot movement, and therefore arguably of limited applicability to images obtained from human ego-motion. Also employing vertical lines as features, this time from omni-directional images, Tang et al. used estimated position differences between sequences to perform  robot navigation \cite{Tang2001}. To make the inference more robust, they used recorded odometry at training time. This approach would certainly reduce the error in the localisation task. However, it could lead to \textit{solving} the training  route, without truly analysing the performance of feature matching methods. Furthermore, without ground truth available in a crowdsensing setting, the technique of training with ground truth is of limited usability.  On the other hand, with many passes through the same space, the reference for a journey could be the visual paths themselves. In this case, one would use ground truth -- if available -- only to ascertain the accuracy of proposed matching or localisation methods. This is the approach taken in the current work.

The performance of previously reported methods that use a retrieval-type approach, albeit of the order of tens of cm, cannot be taken as representative for the evaluation of the methods presented in this chapter. The reviewed publications report results in routes of a few metres in length. The evaluation in this work is in a dataset three orders of magnitude longer.  Deliberately, tracking, such as Kalman filtering is excluded, since it can often hide poor performance of the visual processing. 

\subsection{Crowdsourcing visual paths}
\label{subsec:visual_paths}

Using image sequences represents a particularly data-intensive form of crowdsensing in which the image streams  from wearable cameras could be volunteered to others as reference paths for indoor journeys. An illustration of this concept is presented in Figure \ref{fig:visualpaths}. 

This type of crowdsensing approach is gaining interest, with remarkable work from Google's indoor localisation systems and crowdsourced sensor information and maps~\cite{Kadous2013}. In terms of a retrieval-based visual localisation system, the NAVVIS team ~\cite{Huitl2012} released a dataset for evaluating indoor navigation from a camera-equipped robot. They also advanced earlier work on visual localisation based on matching of SIFT descriptors~\cite{Park2008} to one using a bag of features that could be stored in mobile phones for quick retrieval~\cite{Schroth2011,Schroth2012}. The dataset I introduce in this work is not constrained to robot navigation, as it includes the ego-motion associated with hand-held and wearable devices.

\begin{figure}[t]
\includegraphics[width=\linewidth]{./gfx/Chapter04/map_and_legend.pdf}\label{fig:visualpathsA}
\caption{Maps of the recording locations.}
\label{fig:map_and_legend}
\end{figure}


\begin{figure}[t]

\begin{center}
\includegraphics[width=\linewidth]{./gfx/Chapter04/corridor.pdf}
\caption{ A sample path (Corridor 1, C1) illustrating the multiple passes through the same space. Each of these passes represents a sequence that is either stored in a database, or represents the queries that are submitted against previous journeys. In the assistive context, the user at point A could be a blind or partially sighted user, and he or she would benefit from solutions to the association problem of a query journey relative to previous ``journey experiences'' along roughly the same path, crowdsourced by $N$ users that may be sighted}
\label{fig:visualpaths}
\end{center}
\end{figure}

\subsection{Alternative methods: non feature-based and sensor merging.} 
Fran\c{c}ois Chaumette's team has  recently explored navigation solutions that do not require geometrical nor pixel intensities' features but use mutual information (MI) as a similarity measure. Their system performs a maximisation of the MI that is directly connected with the motion of the robot. These results, and others that rely on tracking visual features \cite{Se2002} cannot serve as a comparison for the current methods either, as they would hinder the fair evaluation of the visual features in isolation.

For outdoor navigation, the Global Positioning System (GPS) has been in widespread use for many years.  In an \textit{indoor} context, localisation technology is still rapidly evolving~\cite{Shen,Wang2012,Quigley2010}.  Using visual information is towards the higher end of computational complexity, and possibly the lower-end of reliability; one would certainly seek to support this approach with other forms of sensor such as Received Signal Strength Indication (RSSI) data, magnetometers, and tracking algorithms~\cite{Schroth2011,Schroth2012,Quigley2010}.  In the work presented in this thesis, we seek to explore efficient techniques that could be used to index and compare the visual path information gathered by multiple user journeys, and to measure the potential of vision on its own as a localisation mechanism. 
%
\subsection{A biological intuition.} Another source of our motivation for the idea of retrieval-based localisation is supported by the well-characterised biological hippocampal place cells~\cite{burgess2002human} that recognise a location from sensory inputs that include those captured by an animal's eyes. This does  \textit{not} suggest that techniques based on optical flow are not relevant: rather, the striking conclusion from recent research is that multiple approaches to visual location inference are at work in biological systems, including optic flow~\cite{Layton2014}, and other mechanisms that may not explicitly involve brain areas specialised in visual motion computation~\cite{hartley2014space}. I will link the findings of the present chapter with this biological intuition in Chapter \ref{ch:chapter5}, where we will propose a model of the hippocampal place cells for indoor localisation.


%------------------------------------------------------------------------- 
\section{Methods}
\label{sec:methods}

In this section I describe the different image description modalities that were evaluated, starting by the feature extraction process, followed by the descriptor quantisation and distance metric used.

\subsection{Pipeline}

I evaluated the performance of several approaches to matching image queries taken from one visual path against the remainder of the visual paths.  In order to index and query the visual path datasets, we adopted a sequence of processes that is illustrated in Figure \ref{fig:FigPipeline}.  The details behind each of the processes (e.g. gradient estimation, spatial pooling) are presented in Section \ref{sec:descriptors}.  Both descriptors that operate on \textit{single} frames (spatial) and descriptors that operate on \textit{multiple} frames (spatio-temporal) were compared.

\begin{figure}
\begin{center}
\includegraphics[width=\textwidth]{./gfx/Chapter04/pipeline.pdf}
\caption{The stages in processing image sequences from database and query visual paths are illustrated above.  This does not show the process behind the estimation of ground truth for the experiments, which is described separately in Section \ref{sec:exp_methods}.  Variants of the gradient and pooling operators, quantisation approaches and distance metrics are described in Section \ref{sec:methods}.}
\label{fig:FigPipeline}
\end{center}
\end{figure}

\subsection{Frame-Level Descriptor}

Inspired by the use of optical flow in motion estimation~\citep{Weickert2006} and space-time descriptors in action recognition \citep{Wang2009} I estimated in-plane motion vectors using a simple approach.  Derivative filters were applied along $(x,y,t)$ dimensions, yielding a 2D$+t$, i.e. spatio-temporal, gradient field.  To capture variations in chromatic content from the visual sequence, spatio-temporal gradients were computed separately for each of the three RGB channels of the pre-processed video sequences.  This yielded a $3\times 3$ matrix at each point in space, effectively a \textit{chromatic Jacobian} (Eq.~\ref{eq:CJ}).  Temporal smoothing was applied along the time dimension, with a support of 11 neighbouring frames. Finally, the components of the matrix were each averaged (pooled) over 16 distinct spatial regions (see Figure~\ref{fig:pooling4lwcolor}), not very dissimilar to those to be described later in this thesis. 



\begin{equation}
\mathbf{J} = \left (
\begin{array}{ccc}
\frac{\partial I_r}{\partial_x} & \frac{\partial I_r}{\partial_y}   & \frac{\partial I_r}{\partial_t} \\
\frac{\partial I_g}{\partial_x}   & \frac{\partial I_g}{\partial_y}  &  \frac{\partial I_g}{\partial_t} \\
\frac{\partial I_b}{\partial_x}  & \frac{\partial  I_b}{\partial_y}  &  \frac{\partial I_b}{\partial_t} 
\end{array} 
\right )
\label{eq:CJ}
\end{equation}


\begin{figure}
\centering
\includegraphics[width=\linewidth]{./gfx/Chapter04/pooling_lwcolor.png}
\caption{A maximum projection intensity rendering of 16 pooling regions over space. The $x$ components of descriptor component time series from regions $A$, $B$, $C$ and $D$ are shown in Figure~\ref{fig:Traces}}
\label{fig:pooling4lwcolor}
\end{figure}


The rationale is that ego-motion is likely to be an important visual cue in human pedestrian navigation, particularly in close or cluttered spaces. Rather than explicit computation of optical flow, the collection of gradient information over different pooling regions of space captures and encodes relevant motion in the form of time series.  The 16 proposed pooling regions illustrated in Figure \ref{fig:kernel} were selected to capture elements of the chromatic space-time Jacobian within two sectors at different radii from the centre of the frame (e.g. A and C, and B and D); to capture information in pairs of regions (such as A and B) that are diametrically opposed from each other horizontally, and also vertically.  Diagonal regions were also proposed, yielding fairly uniform angular coverage over image space. Spatial and temporal gradients in the centre of the frame were not encoded because this region contains less visible motion over equivalent time scales to the more peripheral regions.  


For each visual path, this yielded a total of $16\times 9 = 144$ separate time-series, or signals, of length approximately equal to the video sequences.  An illustration of the time series for one visual path is shown in Figure~\ref{fig:Traces}. Effectively, for each frame, a 144-dimensional descriptor is captured. The elements of this descriptor describe the weighted average of the Jacobian elements within the respective pooling regions that can be used for indexing and matching visual path locations.

\begin{figure}
\begin{center}
	\setlength\figureheight{0.5\linewidth}
	\setlength\figurewidth{0.9\linewidth}
		\input{gfx/Chapter04/tikz/color_chan_comparison.tex}
%\includegraphics[width=\linewidth]{./gfx/Chapter04/Lobe1and6_red_green.pdf}
\caption{Four (of 144) representative signals acquired from a visual path; these signals encode changes in red and green channels as a user moves through space.  The collection of signal traces at one point in time can be used to build a simple frame-level space-time descriptor: LW-COLOR. The signal amplitudes are spatially pooled temporal and spatial gradient intensities.}
\label{fig:Traces}
\end{center}
\end{figure}

At each point in time, the values over the 144 signal channels are also captured into a \textit{single} space-time descriptor per frame: 
LOR.  Our observations from the components of this descriptor are that a) relative ego-motion is clearly identifiable in the signals; b) stable patterns of motion may also be identified, though changes in the precise trajectory of a user could also lead to perturbations in these signals, and hence to changes in the descriptor vectors. Minor changes in trajectory might, therefore, reduce one's ability to match descriptors between users.  These observations, together with the possibility of partial occlusion, led us to the use of \textit{patch} based descriptors, so that multiple descriptors would be produced for each frame. These are introduced next.


\subsection{Local descriptors}
\label{sec:descriptors}

\subsubsection{Keypoint based SIFT (KP-SIFT).}

The original implementation of Lowe's SIFT descriptor follows the extraction of interesting points in the image that are stable to certain transformations, the ``SIFT keypoints'' \cite{Lowe2004}. This is widely used across many branches of computer vision, from object recognition to motion detection and SLAM. I used the standard implementation from VLFEAT \cite{Vedaldi2008} to compute $\vec{\nabla}f(x,y;\sigma)$ where $f(x,y;\sigma)$ represents the embedding of image $f(x,y)$ within a Gaussian scale-space at scale $\sigma$. The parameter \emph{PeakThresh}, $t_p$ is used to filter out small local maxima in scale-space that might be originated by noise. Given the small size of the frames in our sequences the minimum threshold $t_p$ was set to $0$.

\subsubsection{Dense SIFT (DSIFT).}

The Dense-SIFT (DSIFT) descriptor \citep{Lazebnik2006} is a popular  alternative to keypoint based SIFT. It sacrifices some invariance properties available with keypoint-based SIFT, producing descriptors that are densely, rather than sparsely, distributed across the image. This DSIFT descriptor was calculated by  sampling of the smoothed estimate of $\vec{\nabla}f(x,y;\sigma)$.  The implementation of the VLFEAT toolbox was chosen, setting $\sigma = 1.2$, with a stride length of 3 pixels. This  yielded around $2,000$ descriptors per frame, each describing a patch of roughly $10 \times 10$ pixels.

\subsubsection{Single Frame Gabor descriptors (SF-GABOR).}
\label{sec:sf-gabor}

An alternative single frame technique based on a tuned, odd-symmetric Gabor-based descriptor is the SF-GABOR. For this, I used our $8$-directional spatial Gabor filters previously tuned on PASCAL VOC data \cite{Everingham2009} in order to provide an implicit encoding of the orientation of local image structures.  Each filter gives rise to a filtered image plane, denoted $\mathbf{G}_{k,\sigma}$.  For each plane, I compute the discrete spatial convolution, $\mathbf{G}_{k,\sigma} \ast {\Phi}_{m,n}$, with a series of pooling functions, ${\Phi}_{m,n}$. The latter are produced by spatial sampling of the function:

\begin{equation}
\Phi(x,y;m,n) = e^{-\alpha \left [\log_e \left ( \frac{x^2+y^2}{d_n^2}\right ) \right ]^2 - \beta |\theta-\theta_m | }
\label{eq:pool1}
\end{equation}

\noindent with $\alpha = 4$ and $\beta = 0.4$. The values of $m$ and $n$ were chosen to produce 8 angular regions ($m = 0, 1, ...., 7$) at each of two distances $d_1, d_2$ away ($n=1,2$) from the centre of a spatial pooling region. These lobes were similar to those shown in Figure~\ref{fig:pooling4lwcolor}, with one additional central lobe, and used a spatial weighting pattern similar to the DAISY descriptor \cite{Winder2009}. For the central region, corresponding to $m=0$, there was no angular variation but instead a log-normal radial decay, with a limiting value at $(x,y)=(0,0)$. This arrangement yielded a total of  17 spatial pooling regions (see ``poolers'' layer in Figure~\ref{IsoPool}). The resulting $17 \times 8$ fields are sub-sampled to produce dense 136-dimensional descriptors, each representing an approximate $10 \times 10$ region, and yielding around 2,000 descriptors per image frame after spatial sub-sampling. 

\begin{figure}[t]
\centering
\includegraphics[width=0.7\linewidth]{./gfx/Chapter04/Layers.pdf}
\caption{The spatial pooling pattern used for single frame Gabor filtering is based on the regions shown here.  These regions were generated by sampling Eq.~(\ref{eq:pool1}) to create pooling masks. The masks can be applied to the Gabor filtered video frame outputs by spatial convolution, followed by sub-sampling the output every 3 pixels. See text for further details.}
\label{fig:IsoPool}
\end{figure}

These poolers were suggested by Alexiou for visual object recognition \cite{Alexiou2013}, but were thought to be good a good choice for location estimation because of the nature of the RSM navigation dataset (see Section \ref{sec:Dataset}).

In Chapter \ref{ch:chapter5} I provide another formulation for the SF-GABOR descriptor that makes use of a tensor notation. This will be needed to understand a convolutional neural network (CNN) interpretation of artificial place cell models based on the SF-GABOR descriptors.

\subsection{Space-Time descriptors.}
%We therefore chose to explore a denser sampling of the image through capturing patterns over space-time patches in a dense fashion.  We tried three approaches to achieve this.  These are a) the HOG3D descriptor from \cite{Klaser2008}; and two novel techniques, one b) based on antisymmetric Gabor functions of space and time; and one c) incorporating smoothing in time, and gradients in space.  These approaches are somewhat similar to that described in Section \ref{sec:ODPF}, in the sense of using spatial derivatives and incorporating information across time, increasing description accuracy at the expense of computational load (see Table~\ref{table:comput_load}).  A further point to note is that the methods in this section do not make use of colour, using grey-scale information only from the intensity channel of HSI space. Although discarding an important potential visual cue of location, this allowed us to compare performance against a previously described method of HOG3D, providing some confidence that the new techniques are at least comparable to methods already in use.  
%
%The three approaches described in this section require two additional processes. First, an explicit calculation of the direction of the space-time, or gradient vector, that encodes visual motion is carried out.  Secondly, to reduce the information obtained from a dense collection of descriptors so as to make frame-by-frame query possible, a dictionary-learning approach was taken to map descriptors to atoms, then to map atoms to histograms for the database and queries.  This increased complexity does not greatly affect run-time queries when efficiently implemented, but does contribute to on-line vocabulary building and encoding in order to enable efficient performance at query time.
%


Given the potential richness available from space-time information, we explored three distinct approaches to generate space-time patch descriptors.  When generating the descriptor associated with each patch, all  approaches yield multiple descriptors per frame, and all take into account neighbouring frames in time.  In contrast to a sparse-sampling approach of a keypoint-based descriptor, all three densely sample the video sequence.  The three methods are i) HOG 3D~\cite{Klaser2008}; ii) a space-time, antisymmetric Gabor filtering process (ST-GABOR); and iii) a Spatial Derivative, Temporal Gaussian (ST-GAUSS) filter.

\begin{enumerate}
\item The \textbf{HOG 3D} descriptor (HOG3D) \citep{Klaser2008} was introduced with the aim of extending the very successful two-dimensional histogram of oriented gradients technique \citep{Dalal}, to space-time fields, in the form of video sequences.  HOG 3D seeks computational efficiencies by smoothing using box filters, rather than Gaussian spatial or space-time kernels.  This allows three-dimensional gradient estimation across multiple scales using {\em integral video} representations, a direct extension of the integral image idea \citep{Viola2001}.  The gradients from this operation are usually performed across multiple scales.  I used the dense HOG 3D option from the implementation of the authors, and the settings yielded approximately 2,000  descriptors per frame of video. Each descriptor contained 192 elements.

%\subsubsection{The HOG3D Descriptor}
%The 3D gradient estimation on the HOG3D feature is done by applying derivative filters along the three main dimensions of the video $(x,y,t)$. The resultant gradient vector is projected to unit vectors in a 3D quantised space (20 faces of an icosahedron). Similar to the HOG3D features, an odd symmetric Gabor filter is used on each video dimension to compute the 3D gradient vector. Projection to 12 directions in 3D space (dodecahedron) is used to quantise the motion vector in space-time. From this, a descriptor is produced.  We used the author's implementation, described in \cite{Klaser2008}.


\item \textbf{Space-time Gabor (ST-GABOR)} functions have been used in activity recognition, structure from motion and other applications \cite{Bregonzio2009}.  One dimensional convolution was performed between the intensity video-sequence $I(x,y,t)$ and three one-dimensional Gabor functions along either one spatial dimension i.e.\ $x$ or $y$, or along $t$ (see Eq.~\ref{eq:convolutions}).  The one-dimensional convolution  is crude, but appropriate if the videos have been downsampled. The parameters of $\sigma_x$ and $\sigma_y$ were set to be equal, and to provide one complete cycle of oscillation over approximately 5 pixels of spatial span, both for the $x$ and $y$ spatial dimensions. The filter for the temporal dimension was set to provide  around one oscillation over 9 frames.  I also explored symmetric Gabor functions, but found them rather less favourable.


\begin{equation}
\begin{array}{c}
I_1(x,y,t) = I(x,y,t) \ast g_x(x;\sigma_x) \\

I_2(x,y,t) = I(x,y,t) \ast g_y(y;\sigma_y) \\

I_3(x,y,t) = I(x,y,t) \ast g_t(t;\sigma_t)

\end{array}
\label{eq:convolutions}
\end{equation}


After performing the three separate filtering operations, each pixel of each frame is assigned a triplet of values corresponding to the result of each filtering operation.  The three values are treated as being components of a 3D vector.  Over a spatial extent of around $16 \times 16$ pixels taken at the central frame of the 9-frame support region, these vectors contribute weighted votes into descriptor bins according to their azimuth and elevations, with the weighting being given by the length of the vector.  The votes are also partitioned according to the approximate spatial lobe pattern illustrated in Figure~\ref{fig:IsoPool}. Each frame had approximately 2,000 ST-GABOR descriptors, each of 221 elements.



%The components $(I_1,I_2,I_3)$ now represent each pixel in each frame in a three-dimensional feature space. Magnitude-weighted votes are then placed into the two bins corresponding to the elevation and azimuthal angles, respectively, of this feature vector.   The information in these bins is then collected within each of 17 lobes over a fairly small patch size of $16\times 16$ pixels in spatial extent in a single reference frame at the centre of the temporal support region.   These lobes were similar to those shown in Figure ~\ref{fig:kernel}, with one additional central lobe, and used a spatial weighting pattern similar to the DAISY descriptor \cite{Winder2009}.  The resulting descriptor, produced for each patch of image space, is only slightly larger than that based on the colour descriptor described in Section \ref{sec:ODPF}.  However, the descriptors are computed {\it densely} over space, by sampling patches across the image on a regular grid.  Thus, just over 2000 of these 192-dimensional descriptors are produced per frame.
%


\item A final variant of space-time patch descriptor was designed.  This consisted of spatial derivatives in space, combined with smoothing over time \textbf{(ST-GAUSS)}.  In contrast to the strictly one-dimensional filtering operation used for the ST-GABOR descriptor, we used two $5\times 5$ gradient masks for the $x$ and $y$ directions based on derivatives of Gaussian functions, and an 11-point Gaussian smoothing filter in the temporal direction, using a standard deviation of 2.  8-directional quantisation was applied to the angles of the gradient field, and a voting process incorporating gradient magnitude was used to distribute votes across the bins of a 136-dimensional descriptor.  Like the ST-GABOR descriptor, pooling functions, similar to those shown in Figure~\ref{fig:IsoPool}, were applied.  The number of descriptors produced was the same as for the other methods using patch-level descriptions.

%\subsubsection{Spatial Derivative, Temporal Gaussian (ST-GAUSS)}
%A final variant of descriptor was designed. In this case, we tried smoothing in time, rather than taking a derivative, then computing simple derivatives over space using a pair of directional derivative-of-Gaussian spatial filters of size ($5\times 5$). The temporal smoothing was provided by an 11-point Gaussian smoothing filter with a standard deviation of 2.  This was applied before the two dimensional spatial-derivative masks.
%Once the gradient field was estimated, the gradient direction was calculated and quantised into 4 directions, with a sign calculation being used to generate 8 directions.  A simple Parzen-type operation smoothed the angles into a weighted histogram of spatial orientations.
%The descriptor was then constructed by using pooling regions that are very similar to those described above, yielding a 192-dimensional histogram. Finally, the patches of each frame were densely sampled, to produce around 2000 descriptors per frame.


\end{enumerate}


\subsection{Quantisation and histogram encoding}

An initial conjecture was that whole frames from a sequence could be indexed compactly, using the single-frame descriptor (LW-COLOR).  This was found to lead to disappointing performance (see Section~\ref{sec:ch4results}). For the case of many descriptors-per-frame i.e.\ descriptors that are patch-based, there is the added problem of generating around 2,000 descriptors per frame, if dense sampling is used.  Vector quantisation (VQ) was applied to the descriptors, then histograms of quantised descriptors were used to encode each frame as a histogram of visual words \citep{Csurka2004}. The dictionary was always built by excluding the entire journey from which queries are to be taken.  

The resulting dictionaries were then used to encode the descriptors of the $M-1$ training passes and the remaining query pass. Two different approaches to the encoding of descriptors were taken, one based on standard $k$-means, using a Euclidean distance measure (hard assignment, ``HA''), and one corresponding to the Vector of Locally Aggregated Descriptors (VLAD) \citep{Arandjelovic}. These histograms were all $L_2$-normalised.  

In the HA case, the dataset was partitioned by selecting $M-1$ of the $M$ video sequences of passes through each possible path. These $M-1$ sequences have a total of $N$ frames. A dictionary of visual words was created by running the $k$-means algorithm on the partitioned set of training descriptors contained in the $N$ frames. The dictionary size was fixed to 4,000 in order to achieve a balance between computational time and atom stability, and allowing comparison with the work of others in related fields \cite{Chatfield2011}.

For VLAD, a $k$-means clustering was first performed using a dictionary size of 256 words. For each descriptor, sums of residual vectors were used to improve the encoding.  Further advances to the basic VLAD, which include different normalisations and multiscale approaches, are given by \cite{Arandjelovic}. 

\subsection{Localisation using histogram distances}
\label{sec:kernel_encodings}
Once histograms had been produced, a distance measurement was used to compare the similarity of histograms in a query frame with the database entries.  The query operation was simply performed by using the kernel approaches described in \cite{Vedaldi2010}.  Concretely, to compare encodings, either $\chi^2$ or Hellinger distance metrics \citep{Vedaldi2012} were used to retrieve results for HA and VLAD encoding approaches respectively. Distance comparisons were performed directly between either hard assigned Bag-of-Words (BoW) or VLAD image encodings arising from collections of descriptors for each frame. For the $M-1$ videos captured over each path in the database, the queries were constructed from the remaining path.   Each query frame, $H_q$, resulted in $M-1$ separate comparison vectors containing scores.  By using these kernel-based comparisons (which are always positive, and act in the opposite way of a distance metric), we identified the best matching frame, $\hat{f}$, from pass, $\hat{p}$, across all of the $M-1$ vectors.  This may be expressed as: 
\begin{equation}
L(\hat{p},\hat{f}) = \underset{p,f}{\textrm{argmax}} \lbrace K_{D}(H_q,H_{p,f})\rbrace
\label{eq:argmax}
\end{equation}
where $H_{p,f}$ denotes the series of normalised histogram encodings, indexed by $p$ drawn from the $M-1$ database passes, and $f$ denotes the frame number within that pass. $K_D$ denotes the so-called ``kernelised'' version of distance measure \cite{Vedaldi2010}.  To measure the localisation error, we used the ground-truth estimates that were acquired at the same time as the videos. The estimated position of a query, $L$, was simply taken to be that of the best match given by Eq. (\ref{eq:argmax}).  However, in a more robust implementation, checks could be done that would require similar matches in neighbouring frames, both in query and pass.
%%END EDIT BLOCK


%------------------------------------------------------------------------- 
\section{The RSM dataset}
\label{sec:Dataset}
In order to allow different approaches to be compared, and as a community resource to develop this technique, the \textit{RSM dataset} is made publicly available at \url{http://rsm.bicv.org} \cite{Rivera-Rubio2014}.


\subsection{Existing Datasets}

Datasets for evaluating visual localisation methods have often been limited to demonstrate the performance of particular metrics such as point cloud accuracy~\cite{Huitl2012, nardi2014introducing}. This has led to a number of datasets that were difficult to adapt to new work and different performance metrics, or simply unavailable because they were not released to the community~\cite{Matsumoto1996,Ohno1996,Tang2001}.

\paragraph{Historical Datasets}

Early work described in Section \ref{subsec:early_works} used custom-planned datasets for their specific evaluation objectives. This led to datasets \cite{Matsumoto1996,Ohno1996,Tang2001} containing very short sequences, of few meters of length, that could not be used to assess localisation performance at realistic scale of indoor human navigation.

\paragraph{SLAM datasets and the NAVVIS Dataset}

SLAM datasets, found in the robotics community, have a variety of scopes and recorded distances: large indoor spaces \cite{sturm12iros}, outdoor itineraries \cite{Bosse2004}, and up to the scale of a few km car ride \cite{Simpson2011}. They are also heterogeneous in terms of the precision and nature of the ground truth: some use GPS, others the Microsoft Kinect to capture depth \cite{sturm12iros}, while others use the Vicon motion capture system. While the ground truth is often precise (up to the level of GPS, Kinect or Vicon precision), these have usually targeted outdoor comparisons; indoor comparisons focused at geometric reconstruction or pose estimation rather than localisation.

To the best of our knowledge, with the exception of NAVVIS, SLAM datasets have had rather restricted distances, not addressing real-world navigation on the scale of large buildings. The NAVVIS project described in Section \ref{subsec:visual_paths} first introduced a more generic dataset that could evaluate visual localisation and navigation at human scale for robotic applications. The RSM dataset takes the evaluation and the principle closer to the assistive context than the robot-centric approach of the NAVVIS team: our data and evaluation context introduces the particularities of human motion, both from hand-held and a wearable camera.

\subsection{The RSM dataset of visual paths}

I have previously defined a ``visual path'' as the video sequence captured by a moving person in executing a journey along a particular physical path. For the construction of our dataset, the \textit{RSM dataset of visual paths}, a total of 60 videos were acquired from 6 corridors of a large building. In total, 3.05 km of data is contained in this dataset at a natural indoor walking speed.  For each corridor, ten passes (i.e.\ 10 separate visual paths) are obtained; five of these are acquired with two different devices with 30 videos each. One device was a LG Google Nexus 4 phone running Android 4.4.2.  The video data was acquired at approximately 24-30 fps at two different resolutions, $1280 \times 720$ and $1920\times 1080$ pixels.  The second device was a Google Glass (Explorer edition) acquiring at a resolution of $1280 \times 720$, and at a frame rate of 30 fps. Table~\ref{tbl:Datasets} summarises the acquisition.  As can be seen, the length of the sequences varies within some corridors, due to a combination of different walking speeds and/or different frame rates. Lighting also varied, due to a combination of daylight/night-time acquisitions, and occasional prominent windows that represent strong lighting sources in certain parts of some corridors.  Changes were also observable in some videos from one pass to another, due to the presence of changes and occasional appearance from people. In total, more than 90,000 frames of video are labelled with positional ground truth in a path relative manner. 

\subsection{Ground truth Acquisition}

A surveyor's wheel (Silverline) with a precision of 10 cm and error of $\pm 5\%$ was used to record distance, but was modified by wiring its encoder to a Raspberry Pi running a number of measurement processes. The Pi was synchronised to network time using the network time protocol (NTP) enabling synchronisation with timestamps in the video sequence.  Because of the variable-frame rate of acquisition, timestamp data from the video was used to align ground-truth measurements with frames. This data was used to access the accuracy of associating positions along journeys through frame indexing and comparison.


\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{./gfx/Chapter04/rsm_collage.jpg}
\caption{A ``collage'' depicting a thumbnail of each corridor of the RSM dataset. From top left to bottom right C1, C2, ..., C6.}
\label{fig:IsoPool}
\end{figure}


\begin{table}[ht]
\caption{A summary of the dataset with thumbnails}
\label{tbl:Datasets}

\begin{center}

\centering
    \begin{tabular}{l c c c c c c c}
    \hline
    & \multirow{2}{*}{\bf{Photo}} & \multicolumn{3}{c}{\bf{Length (m)}} & \multicolumn{3}{c}{\bf{No. of frames}}  \\ \cline{3-8}
             & ~     & Avg    & Min   & Max   & Avg              & Min  & Max  \\ \hline
    C1       & \begin{minipage}{.2\textwidth}
      			\includegraphics[width=\linewidth]{./gfx/Chapter04/table/1.jpg}
			   \end{minipage}
			        & 57.9  & 57.7 & 58.7 & 2157         & 1860 & 2338 \\ \hline
    C2       & \begin{minipage}{.2\textwidth}
      			\includegraphics[width=\linewidth]{./gfx/Chapter04/table/2.jpg}
			   \end{minipage}
         & 31.0  & 30.6 & 31.5 & 909          & 687  & 1168 \\ \hline
    C3       & \begin{minipage}{.2\textwidth}
      			\includegraphics[width=\linewidth]{./gfx/Chapter04/table/3.jpg}
			   \end{minipage}
			        & 52.7  & 51.4 & 53.3 & 1427         & 1070 & 1777 \\ \hline
    C4       & \begin{minipage}{.2\textwidth}
      			\includegraphics[width=\linewidth]{./gfx/Chapter04/table/4.jpg}
			   \end{minipage}
			        & 49.3  & 46.4 & 56.2 & 1583         & 1090 & 2154 \\ \hline
    C5       & \begin{minipage}{.2\textwidth}
      			\includegraphics[width=\linewidth]{./gfx/Chapter04/table/5.jpg}
			   \end{minipage}
			        & 54.3  & 49.3 & 58.4 & 1782          & 1326 & 1900 \\ \hline
    C6       & \begin{minipage}{.2\textwidth}
      			\includegraphics[width=\linewidth]{./gfx/Chapter04/table/6.jpg}
			   \end{minipage}
			        & 55.9  & 55.4 & 56.4 & 1471          & 1180 & 1817 \\ \hline \hline
	\multicolumn{2}{l}{Total}      & \multicolumn{3}{c}{3.042 km} & \multicolumn{3}{c}{90,302 frames} \\ \hline
    \end{tabular}

\end{center}
\end{table}


\section{Experiments}
\label{sec:exp_methods}

\subsection{Performance Evaluation}

The methods for a) describing spatial or space-time structure, b) indexing and comparing the data are summarised in Table~\ref{tbl:Methods}. The choice of parameters was selected to allow a) as consistent a combination of methods as possible, allowing fair comparisons of the effect of one type of encoding or spatio-temporal operator to be isolated from others b) to select parameter choices close to other research in the area, e.g.\ for image categorisation, the chosen dictionary sizes of $\approx 256$ and $\approx 4000$ words are common. 

\begin{table}
\caption{A summary of the different encoding methods and their relationships to different descriptors. The number of elements of each descriptor is also reported (\textbf{Dim}).}
\label{tbl:Methods}
\small
\centering
    \begin{tabular}{l p{0.5cm} p{0.9cm} p{0.5cm} p{2.5cm} c }
    \hline
    \textbf{Method}              & \textbf{ST} & \textbf{Dense} & \textbf{Dim}  & \textbf{Encoding}      & \textbf{Metric}    \\ \hline
    \multirow{2}{*}{KP-SIFT} & \multirow{2}{*}{No} & \multirow{2}{*}{No} & \multirow{2}{*}{128}     & HA-4000  &   $\chi^2$    \\ \cline{5-6} 
        ~                   & ~      & ~           & ~      & VLAD-256 & Hellinger \\ \hline
    \multirow{2}{*}{DSIFT} & \multirow{2}{*}{No} & \multirow{2}{*}{Yes} & \multirow{2}{*}{128}     & HA-4000  &   $\chi^2$    \\ \cline{5-6}
    ~                   & ~      & ~           & ~      & VLAD-256 & Hellinger \\ \hline
    \multirow{2}{*}{SF-GABOR} & \multirow{2}{*}{No}               & \multirow{2}{*}{Yes}  & \multirow{2}{*}{136}  & HA-4000  & $\chi^2$      \\  \cline{5-6}
    ~                   & ~      & ~           & ~      & VLAD-256 & Hellinger \\ \hline
    LW-COLOR           & Yes & No & 144           & N/A       \\ \hline
    \multirow{2}{*}{ST-GABOR}          & \multirow{2}{*}{Yes}              & \multirow{2}{*}{Yes}  & \multirow{2}{*}{221}  & HA-4000  & $\chi^2$      \\  \cline{5-6}
    ~                   & ~      & ~           & ~      & VLAD-256 & Hellinger \\ \hline
    \multirow{2}{*}{ST-GAUSS}           & \multirow{2}{*}{Yes}              & \multirow{2}{*}{Yes}  & \multirow{2}{*}{136}  & HA-4000  & $\chi^2$      \\  \cline{5-6}
    ~                   & ~      & ~           & ~      & VLAD-256 & Hellinger \\ \hline
    \multirow{2}{*}{HOG3D}               & \multirow{2}{*}{Yes}              & \multirow{2}{*}{Yes} & \multirow{2}{*}{192}   &  HA-4000  & $\chi^2$      \\  \cline{5-6}
    ~                   & ~       & ~          & ~      & VLAD-256 & Hellinger \\ \hline
    \end{tabular}
    \normalsize
\end{table}


\subsubsection{Error distributions}
\label{sec:CDFs}
Error distributions allow us to quantify the accuracy of being able to estimate {\em locations} along physical paths within the RSM dataset described in Section~\ref{sec:Dataset}. To generate the error distributions, we did the following: 

Using the kernels calculated in Section~\ref{sec:kernel_encodings}, location inference was attempted. One kernel is shown in Figure~\ref{fig:kernel}, where the rows represent each frame from the query pass, and the columns represent each frame from one of the remaining database passes of that corridor. The values of the kernel along a row represent a ``score'' between a query and different database frames given by applying the kernel mapping functions described in \citep{Vedaldi2012} for the $\chi^2$ and Hellinger kernel depending on the case. In this experiment, the position of the best matching database image to the query frame was calculated. The error between this and the ground truth $\epsilon$, in m, is then determined. We used the ground-truth information acquired as described in Section \ref{sec:Dataset}. 

In order to characterise the reliability of such scores, bootstrap estimates of error distributions were obtained by using 1 million trials. Specifically, permuting the paths that are held in the database and randomly selecting queries from the remaining path, these error distributions in localisation can be obtained. The bootstrap estimates consisted of repeated runs with random selections of groups of frames, revealing the variability in these estimates, including that due to different numbers of paths and passes being within the database. In Appendix \ref{appendixCDF} we describe the algorithm to generate the cumulative distribution functions (CDFs) in more detail. The distribution of the errors gives us a probability density estimate, from which one can get the cumulative distribution function (CDF) $P(x \leq |\epsilon|)$. 

The outcome is shown in Figure~\ref{fig:CDFall}, where the variability in the lines indicate the range of the results obtained during permuting the observations as described above.

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{./gfx/Chapter04/kernel.pdf}
\caption{Example of a $\chi^2$ kernel produced by hard assignment and using the SF-GABOR descriptors when querying with pass 1 of corridor 2 against a database comprised of passes 2-10.}
\label{fig:kernel}
\end{figure}

If we consider the idea of crowdsourcing journey information from many pedestrian journeys through the same corridors, this approach to evaluating the error makes sense:  all previous journeys could be indexed and held in the database; new journey footage would be submitted as a series of query frames (see Figure~\ref{fig:pathexample} from Chapter \ref{ch:chapter2}).    

All the results were generated with a downsampled version of the videos at $208 \times 117$ pixels. 




%------------------------------------------------------------------------- 
\section{Results}
\label{sec:ch4results}

I calculated the average absolute positional error (in metres) and the standard deviation of the absolute positional errors across the provided dataset, and these are shown in Table~\ref{Table:summaries}. For these errors, all queries, by a leave-one-out strategy, have been used, but there is otherwise no random sampling of the queries.  Standard deviations of the absolute errors are also provided.  Table~\ref{Table:summaries} also provides the Area-Under-Curve (AUC) values obtained from the CDFs of Figure~\ref{fig:CDFglobal}.


\begin{figure}[h!]
\subfloat[]{\includegraphics[width=0.5\textwidth]{./gfx/Chapter04/CDF_Figs/SF_GABOR.pdf}}	\subfloat[]{\includegraphics[width=0.5\textwidth]{./gfx/Chapter04/CDF_Figs/DSIFT.pdf}}\\
\subfloat[]{\includegraphics[width=0.5\textwidth]{./gfx/Chapter04/CDF_Figs/ST_GABOR.pdf}}
\subfloat[]{\includegraphics[width=0.5\textwidth]{./gfx/Chapter04/CDF_Figs/ST_GAUSS.pdf}}\\
\subfloat[]{\includegraphics[width=0.5\textwidth]{./gfx/Chapter04/CDF_Figs/KPSIFT.pdf}}
\subfloat[]{\includegraphics[width=0.5\textwidth]{./gfx/Chapter04/CDF_Figs/HOG3D.pdf}}
\caption{Comparison between the error distributions obtained with the different methods. Note the high reproducibility of the performance results. The origin of the variability within each curve is explained in Section \ref{sec:CDFs}}
\label{fig:CDFglobal}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{./gfx/Chapter04/CDF_Figs/all.pdf}
\caption{Comparison between the error distributions obtained with the different methods. The results for a random frame test (RANDOM) were introduced as a ``sanity check''}
\label{fig:CDFall}
\end{figure}



%\begin{figure}[h]
%\begin{center}
%\begin{subfigure}[b]{.6\linewidth}
%\captionsetup{skip=0pt} % local setting for this subfigure
%
%\begin{subfigure}[b]{\linewidth}
%
%	\begin{subfigure}[b]{.45\linewidth}
%	\centering
%	\includegraphics[width=\textwidth]{./gfx/Chapter04/CDF_Figs/SF_GABOR.pdf}\caption{}\label{fig:CDFa}
%	\end{subfigure}
%	\begin{subfigure}[b]{.45\linewidth}
%	\centering
%	\includegraphics[width=\textwidth]{./gfx/Chapter04/CDF_Figs/DSIFT.pdf}\caption{}\label{fig:CDFb}
%	\end{subfigure}
%	
%\end{subfigure}
%
%
%\begin{subfigure}[b]{\linewidth}
%
%\begin{subfigure}[b]{.45\linewidth}
%\centering
%\includegraphics[width=\textwidth]{./gfx/Chapter04/CDF_Figs/ST_GABOR.pdf}\caption{}\label{fig:CDFc}
%\end{subfigure}
%\begin{subfigure}[b]{.45\linewidth}
%\centering
%\includegraphics[width=\textwidth]{./gfx/Chapter04/CDF_Figs/ST_GAUSS.pdf}\caption{}\label{fig:CDFd}
%\end{subfigure}
%
%\begin{subfigure}[b]{\linewidth}
%
%	\begin{subfigure}[b]{.45\linewidth}
%	\centering
%	\includegraphics[width=\textwidth]{./gfx/Chapter04/CDF_Figs/KPSIFT.pdf}\caption{}\label{fig:CDFe}
%	\end{subfigure}
%	\begin{subfigure}[b]{.45\linewidth}
%	\centering
%	\includegraphics[width=\textwidth]{./gfx/Chapter04/CDF_Figs/HOG3D.pdf}\caption{}\label{fig:CDFf}
%	\end{subfigure}
%	
%\end{subfigure}
%
%\end{subfigure}
%
%\end{subfigure}%
%\begin{subfigure}[b]{.4\linewidth}
%\centering
%\includegraphics[width=0.95\textwidth]{./gfx/Chapter04/CDF_Figs/CDF_all.pdf}\caption{Comparison between the error distribution obtained with the different methods. The results for a random test (RANDOM) were introduced as a ``sanity check''.}\label{fig:CDFglobal}\label{fig:CDFall}
%\end{subfigure}%
%\caption{Cumulative Distribution Functions of the methods under study.}
%\end{center}
%\end{figure}

\subsection{Localisation error vs ground-truth route positions}

As described in the previous section, by permuting the database paths and selecting, randomly, queries from the remaining path that was left out in the dictionary creation, one can assess the errors  in localisation along each corridor for each pass, and calculate, also, the average error in localisation on a per-corridor basis, or per-path basis.  For these, the ground-truth information acquired as described in Section \ref{sec:Dataset} was used.  Figure~\ref{fig:x_e_vs_gt} provides some examples of the nature of the errors, showing evidence of those locations that are often confused with each other.  As can be seen, for the better method (top trace of Figure~\ref{fig:x_e_vs_gt}) whilst average errors might be small, there are, occasionally, large errors due to poor matching (middle trace). Errors are significantly worse for queries between different devices (see Figure~\ref{fig:x_e_vs_gt}(c)). 

\begin{figure}[h!]
\centering
\subfloat[Corridor 3 using Pass 2 acquired with LG Nexus 4. Results for the best spatio-temporal method, ST-GABOR.]{\includegraphics[width=.6\linewidth]{./gfx/Chapter04/xe_vs_x/ST_GABOR_good.pdf}}
\\
\subfloat[Corridor 3 using Pass 6 acquired with Google Glass. Results for the best single-frame method, SF-GABOR.]{\includegraphics[width=.6\linewidth]{./gfx/Chapter04/xe_vs_x/SF_GABOR_good.pdf}}
\\
\subfloat[Corridor 2 using Pass 6 acquired with Google Glass. Results for the worst overall method, LW-COLOR.]{\includegraphics[width=.6\linewidth]{./gfx/Chapter04/xe_vs_x/Lightweight_bad.pdf}}
\caption{Estimated location vs. ground truth. Illustrative examples of good/bad location estimation performance. a) Uses the best descriptor and a single-device dataset, b) uses the best descriptor and a cross-device dataset and c) uses the worst descriptor, and a multiple-device dataset.}
\label{fig:x_e_vs_gt}
\end{figure}


\subsection{Performance Summaries}
\label{visloc_perf}
I calculated the average of the absolute positional error (in m) and the standard deviation of the absolute positional error in a subset of the complete RSM dataset (Table~\ref{Table:summaries}). I  used a leave-one-journey-out approach (all the frames from an entire journey are excluded from the database).  Using bootstrap sampling, we also estimated the cumulative density functions of the error distributions in position, which are plotted in Figure~\ref{fig:CDFall}. The variability in these curves is not shown, but is summarised in the last two columns of Table~\ref{Table:summaries} through the area-under-curve (AUC) values. In the best case (SF-GABOR), AUCs of the order of $~96\%$ would mean errors generally below 2 m; in the worst (HOG3D), AUCs $\approx$ 90\% would mean errors of around 5 m.  These  mean absolute error estimates are obtained as the queries, the dictionary and the paths in the database are permuted. 


\begin{table}[!h]
\centering

    \begin{tabular}{ c c c c c }
    \hline
  %  Method   &  Encoding & $\mu_{\epsilon}$ &  $\sigma_{\epsilon}$  & min AUC & max AUC \\ \hline
     \multirow{2}{*}{\bf Method}  & \multicolumn{2}{c}{Error summary (\SI{}{m})} & \multicolumn{2}{c}{AUC (\%)}\\ \cline{2-5}    
    & $\mu_{\epsilon}$ & $\sigma_{\epsilon}$ & Min & Max \\ \hline

    SF-GABOR       & \textbf{1.59}             & 0.11             & 96.11   & \textbf{96.39}   \\ \hline
    %SF-GABOR & Hellinger      & 135.1              & 46.5              & 96.29   & 96.71   \\ \hline
	DSIFT           & 1.62              & 0.11  & 95.96   & 96.31   \\ \hline 

	KP-SIFT           & 2.14              & 0.17  & 94.58   & 95.19   \\ \hline 

	LW-COLOR           & 3.64              & 1.13  & 91.42   & 92.25   \\ \hline \hline

    %SIFT     & Hellinger      & 132.7              & 41.4              & 94.34   & 94.95   \\ \hline

	% Spatio temporal
	
	ST-GAUSS        & \textbf{2.11}              & 0.24 & 94.82   & \textbf{95.57}   \\ \hline
    %ST-GAUSS & Hellinger      & 144.1              & 52.4              & 92.69   & 93.47   \\ \hline
    ST-GABOR       & 2.54              & 0.19 & 93.90   & 94.44   \\ \hline
    %ST-GABOR & Hellinger      & 179.5              & 62.3              & 93.98   & 94.60   \\ \hline

    HOG3D         & 4.20              & 1.33              & 90.89   & 91.83   \\ \hline
    %HOG3D    & Hellinger      & 366.5              & 120.3              & 91.49   & 92.37   \\ \hline
    %LW-COLOR & N/A       & 363.9              & 113.2              & 91.42   & 92.25   \\ \hline
    \end{tabular}


\caption{Summaries of average absolute positional error and standard deviation of positional errors for different descriptor types. $\mu_{\epsilon}$ is the average absolute error, and $\sigma_{\epsilon}$ is the standard deviation of the error in metres. Top: single frame methods. Bottom: spatio-temporal methods.}
\label{Table:summaries}
\end{table}

Note that tracking algorithms were not used, and so there is no motion model or estimate of current location given the previous one. As we will describe in Section \ref{sec:slamcomp}, incorporating a particle filter or Kalman filter should reduce the errors, particularly where there are large jumps within small intervals of time. This deliberate choice allows us to evaluate the performance of different descriptor and metric choices independently.

The results show that localisation is achieved with good accuracy in terms of CDF and AUC without a large difference between the applied methods, despite the big diversity in their complexity. Absolute errors show significant differences between methods, with average absolute errors in the range of \SI{1.5}{m} to \SI{4.20}{m}. Single frame methods (SF-GABOR, KP-SIFT and DSIFT) perform slightly better than spatio-temporal approaches. This is not surprising, as the spatio-temporal methods might be strongly affected by the self motion over fine temporal scales.

In spite of using image retrieval methods in isolation, the attainable accuracy appears to be in line with those of  other methods reviewed in Section~\ref{sec:retrieval}.  However, previously reported methods include tracking, the use of other sensors, or estimates of motion. In this work, no form of tracking was used in estimating position: this was deliberate, in order to assess performance in inferring location from the visual data fairly.  Introducing tracking would probably improve localisation performance, and could reduce query complexity. Yet, tracking often relies on some form of motion model, and for pedestrians carrying or wearing cameras, motion can sometimes be relatively unpredictable. In the next section I will describe the comparison of the appearance-based approach with SLAM methods, particularly through comparing performance with large-scale direct monocular SLAM (LSD-SLAM).

%------------------------------------------------------------------------- 
\section{Comparison with SLAM}
\label{sec:slamcomp}


\subsection{Background}


Another important branch of vision based navigation technique can be found in robotics. Visual SLAM \citep{konolige2007frame}, \citep{engelhard2011real},\citep{neira2008guest} (Simultaneous localisation and Mapping) provides a real time reconstruction of the scene by using stereo cameras (stereoSLAM) or a single one (monoSLAM). Though SLAM is often described for its ability to infer a geometric model of a scene, it also estimates a camera trajectory.  The combination of the two is a powerful source of navigation information. In addition, in subsequent journeys along the same route, geometric information can be refined and also used for refinement of camera pose estimates.

Apart from the possible computational load issues that solutions growing in complexity can entail, the main challenge of SLAM, or the ``SLAM problem'' is to produce an accurate map and therefore a precise localisation. Traditionally, this problem has been studied from two different perspectives: from the image processing and recognition perspective (the latest developments including deep learning approaches \citep{chen2014convolutional}; from a tracking point of view, relying in technologies ranging from depth and inertial sensors to wireless networking to minimise the error drift in the location estimations.

%We are more interested in the first perspective, as we believe our methods presented here can be complementary to SLAM and provide a better location inference acting not only as a loop closure method \citep{glover2012openfabmap} but also as a semantic information retrieval system. 

\citeauthor{davison2007monoslam}~\cite{davison2007monoslam} developed a seminal real time mono SLAM method that used a lightweight Harris feature detector of interesting points within the point of view of the camera. In the next step the system would locate the same feature in a neighbouring patch. Montiel and colleagues, as we will see in the comparison with EKF SLAM, took the same monoSLAM approach but aiming for its simplification in degrees of freedom and trying to overcome the problem of the initialisation of features. In \citep{montiel2006unified} they devised an inverse depth parametrisation for point features with 3 degrees of freedom (DOF) instead of the expensive 6.

The complexity of the visual features used for SLAM increased with the introduction of SIFT instead of Harris detectors in \citep{chen2007sift} and \citep{suzuki2011development} to Montiel and Davison's approaches. This opened SLAM to outdoor and more challenging applications as the SIFT invariances increased the robustness of the SLAM.

This robustness is key, as some visual SLAM algorithms provide a navigation method suitable for use by autonomous robots \citep{konolige2007frame} but relying on static features from the scene that are subsequently matched before the trajectory is estimated. However, in real life conditions, many of these features are dynamic, since they belong to objects or elements of the scene that are moving (e.g. a crowded scene). Additionally, SLAM algorithms, particularly visual monoSLAM, rely on optic flow induced by ego-motion in order to infer a geometry and build up a map. In the presence of significant additional motion within the scene, algorithms can begin to fail.  One such failure mode is called scale drift and has been studied and taken into account by \cite{strasdat2010scale}. 

Recent vision-based approaches have been developed by Alcantarilla et al. (\citeyear{alcantarilla2010visual}; \citeyear{alcantarilla2012combining}) They incorporate dense optical flow estimation into visual SLAM in order to improve the performance of algorithms in crowded and dynamic environments by detecting the presence of objects that are moving relative to the world coordinate system. Additionally, they have developed a fast vision-based method to speed-up the association between visual features and points in large 3D databases \citep{alcantarilla2010learning}. This approach consists of learning the visibility of the features in order to narrow down the number of matching point correspondence candidates.

One of the most recent SLAM algorithms is LSD-SLAM~\citep{engel14eccv}. This semi-dense tracking and mapping method seem to perform well in an indoor SLAM setting. This approach, instead of keypoints and descriptors, uses semi-dense depth maps for tracking by direct image alignment. This is a remarkable step forward, as the semi-dense maps allow lighter frame to frame comparisons, to the point where odometry can be performed on a modern smartphone \citep{schoeps14ismar}. This system, as most SLAM methods, relies greatly on a very accurate camera calibration and initialisation routine, and as we will see in the detailed comparison below, best results are often achieved under specific conditions, such as monochrome global shutter cameras with fish eye lenses.

\subsection{Comparative study}

The first attempt to put the appearance-based approach into perspective consisted of applying one implementation of the simultaneous localisation and mapping technique (SLAM) to the RSM dataset, at the same frame resolution as for the appearance-based  localisation discussed in this chapter. I chose the ``EKF Mono SLAM'' \citep{civera20091}, which uses an Extended Kalman Filter (EKF) with 1-point RANSAC.  This implementation was selected for three reasons: a) it is a monocular SLAM technique, so comparison with the single-camera approach is fairer; b) the authors of this package report error estimates -- in the form of error distributions; and c) the errors from video with similar resolutions (240 $\times$ 320) to the small images (208 $\times$ 117) used for the appearance-based matching were reported as being below 2 m for some sequences \citep{civera20091} in their dataset.

The results of the comparison were surprising, and somewhat unsatisfactory. The challenging ambiguity of the sequences in the RSM dataset, and possibly the low resolution of the queries, might explain the results. The feature detector, a FAST corner detector \citep{rosten2006machine}, produced a small number of features in its original  configuration. I lowered the feature detection threshold until the system worked on a small number of frames from each sequence. Even with more permissive thresholds, the average number of FAST features averaged only 20 across the experiments of the RSM dataset. This small number of features led to inaccuracy in the position estimates, causing many of the experimental runs to stop when no features could be matched. The small number of features per frame is also not comparable with the feature density of the methods described in this work, where an average of 2,000 features per frame was obtained for the ``dense'' approaches. Dense SLAM algorithms might fare better, and that was the reason why we chose to study LSD-SLAM, as we will describe in the next section.

\subsubsection{Comparison with state-of-the-art SLAM: LSD-SLAM}
\label{sec:comparisonSLAM}

I compared the best performing appearance-based method, SF-GABOR, against the current state-of-the-art in SLAM for indoor sequences, LSD-SLAM.  I first ran the LSD-SLAM code over the 60 video sequences of the RSM dataset and retrieved the results from the visual odometry engine in order to obtain position estimates for each processed frame.  This provided an estimate of the distance travelled by a user, allowing comparison to the results we had for the appearance-based method. 

The cameras in the devices used to acquire the RSM data are quite different for those suggested for use with LSD-SLAM (recommended: monochrome global-shutter camera with fish eye lens); therefore, the standard LSD-SLAM software was modified to recover from lost tracking when this happened. The semi-dense SLAM parameters were adjusted to adapt best to our conditions and data, specifically, to minimise instances of lost tracking in medium resolution versions of the images, i.e. at a considerably higher resolution than the image required for the appearance-based approach. In \ref{sec:slampars} I provide the parameter values that were used for the experiments.


\subsubsection{Performance of LSD-SLAM on RSM dataset sequences}

Figure~\ref{fig:slamperf} (a) and (b) illustrate the localisation performance of LSD-SLAM (blue) with respect to the ground truth (red). The two cases that are shown were selected to illustrate that accuracy of localisation can vary, depending on the specific corridor within the RSM dataset (see Section~\ref{sec:Dataset}). As can be seen from the figures, in the better case, the absolute error is below \SI{2}{\metre}. However, in some cases, errors rise as high as $5$ m. Perhaps equally importantly, these errors were found when operating at image sizes that allowed LSD-SLAM to function without losing tracking.

\begin{figure}
	\centering
	\subfloat[(a)][Errors obtained from one experiment using LSD-SLAM. The maximum absolute error was just over $1$ m. ]{
	%\setlength\figureheight{0.5\textwidth}	
	%\setlength\figurewidth{0.8\textwidth}
	%\input{./gfx/Chapter04/tikz/lsd_slam_performance_good.tex} %TIKZ
	\includegraphics[width=0.45\textwidth]{./gfx/Chapter04/lsd_slam_good.pdf}
	}
	\quad
	\subfloat[(b)][Errors obtained from another experiment using LSD-SLAM. In this case, the absolute error peaked at around $5$ m. ]{
	%\setlength\figureheight{0.5\textwidth}
	%\setlength\figurewidth{0.8\textwidth}
	%\input{./gfx/Chapter04/tikz/lsd_slam_performance_normal.tex} %TIKZ
	\includegraphics[width=0.45\textwidth]{./gfx/Chapter04/lsd_slam_normal.pdf}
	}
	\caption{Localisation performance in LSD-SLAM; this shows that in different corridors, the accuracy of LSD-SLAM can change quite significantly. See text for details of the SLAM parameters and the nature of the dataset, but note that these were obtained at an image resolution of $1024 \times 576$ pixels. At lower resolutions, loss of tracking dominated the experiments. The difference in the $x$ and $y$ axis labelling is because experiments in a) and b) are obtained from two different corridors with different lengths and different numbers of frames.}
	\label{fig:slamperf}
\end{figure}


\subsubsection{CDF comparisons}

CDFs, as introduced in Section \ref{sec:CDFs} illustrate very well the differences in the performance of methods. As we can see from Figure \ref{fig:cdf} our appearance-based method using Gabor-based descriptors outperforms LSD-SLAM in the RSM dataset. 

Another perspective to these results can be seen from Table~\ref{tbl:cdftable}, where we can see a head to head comparison for the probabilities of achieving a localisation error smaller than $\epsilon$. We can see how SF-GABOR outperforms LSD-SLAM, achieving smaller localisation errors with a larger probability. For the sample of CDF values obtained, we can see that SF-GABOR can localise with an error below 2.5 m in the 95.95\% of the times, whilst LSD-SLAM can only guarantee errors below 2.5 m 59.88\% of the times on average.

This does not imply that the technique that appearance-based technique we used \textit{replaces} SLAM or its equivalents -- it is merely a different context of usage. A user who is interested in getting from $A$ to $B$ may be less interested in mapping the geometry along that route than whether their trajectory matches that of other users who have previously made the same journey. In the scenario described, and in which performance is being compared, the journey of a user is assessed against those made by other people who have made the same journey -- location becomes journey-relative, not map-relative. The usage scenario also means that loop closure may not be possible. However, localisation systems should aim for a combined solution, as LSD-SLAM seems to perform better given an appearance-based loop closure method \citep{engel14eccv} such as FabMap, which could be replaced by any of the solutions proposed \citep{Rivera-Rubio2015PRL} or a combination of both.

%A final point to remember is that the cameras that are used in capturing the journeys are not necessarily identical, and certainly may be uncalibrated.


\begin{table}
\caption{Cumulative distribution function values against localisation error in metres ($\epsilon$). $P(|\epsilon| \leq x)$, expressed as a percentage. From this table, the best appearance method achieves a probability of 90\% of localising with an error below $2$ m, whilst LSD-SLAM achieves just above 50\% accuracy level for that error boundary, and required larger images. In addition, the performance of LSD-SLAM was significantly worse (compare the minimum performance colums) on some corridors and journeys, bringing the overall average down across the RSM dataset relative to appearace based localisation.}
\label{tbl:cdftable}
\centering
    \begin{tabular}{lcccc}
    %\begin{tabular}{lSSSSSS}
    \toprule
    %\hline
    Method & 
    \multicolumn{2}{c}{SF-GABOR} &
    \multicolumn{2}{c}{LSD-SLAM} \\

    $x$ (m) & Ave. $P$  & (min $P$, max $P$ ) & Ave. $P$ & (min $P$, max $P$) \\
    \midrule
    0.25 & 0.23 & (0.22, 0.25) & 0.06 & (0.05, 0.07)  \\
    0.50 & 0.53 & (0.52, 0.55) & 0.15 & (0.14, 0.16) \\
    0.75 & 0.67 & (0.66, 0.69) & 0.23 & (0.21, 0.24) \\
    1.00 & 0.73 & (0.71, 0.74) & 0.29 & (0.28, 0.31)\\
    1.25 & 0.80 & (0.79, 0.82) & 0.36 & (0.34, 0.37)\\
    1.50 & 0.85 & (0.84, 0.86) & 0.41 & (0.40, 0.42) \\
    1.75 & 0.89 & (0.88, 0.90) & 0.47 & (0.46, 0.49) \\
    2.00 & 0.92 & (0.92, 0.94) & 0.51 & (0.50, 0.53)  \\
    2.25 & 0.94 & (0.93, 0.95) & 0.55 & (0.54, 0.57) \\
    2.50 & 0.96 & (0.95, 0.97)  & 0.60 & (0.58, 0.61) \\
    \bottomrule
    \end{tabular}
\end{table}



\subsubsection{Reproducibility errors}

Another comparison is illustrated by Figure \ref{fig:reprod}. These are RSM corridors 1 and 3 (C1 and C3) average reproducibilities for multiple ``leave one out'' passes. These diagrams show how whilst LSD-SLAM reports worse average results in terms of errors it has a localisation error consistency comparable to that of SF-GABOR, and better than that, errors rarely go beyond the 5 meters boundary, with an average of $\mu_e = 2.48 \pm 2.37$ m. Conversely, SF-GABOR present outliers, but for some sequences the error is consistently below 1 m ($\mu_e = 1.31 \pm 0.39$), supporting its suitability for the indoor localisation context.

Box and whisker plots that evaluate the comparative reproducibility in localisation are presented in Figure \ref{fig:reprod}. These illustrate the reproducibility within RSM corridors 1 and 3 (C1 and C3) for multiple ``leave-one-out'' passes. The plots suggest that whilst LSD-SLAM yielded worse results in terms of error, it has a consistency in performance that is comparable to that of SF-GABOR. Also, the errors for LSD-SLAM rarely go beyond $5$ m, with an average of $\mu_e = 2.48 \pm 2.37$ m. Conversely, the appearance based method contains some outliers; even so, for some sequences the error is of the order or lower ($\mu_e = 1.31 \pm 0.39$ m) than the best reported for SLAM, supporting the idea of an appearance-based localisation approach for indoor navigation. 

\begin{figure}
\centering
\subfloat[C1 SF-GABOR]{\includegraphics[width=0.5\linewidth]{./gfx/Chapter04/Corr1RepApp.pdf}}
\subfloat[C3 SF-GABOR]{\includegraphics[width=0.5\linewidth]{./gfx/Chapter04/Corr3RepApp.pdf}}


\subfloat[C1 LSD-SLAM]{\includegraphics[width=0.5\linewidth]{./gfx/Chapter04/Corr1RepSLAM.pdf}}
\subfloat[C3 LSD-SLAM]{\includegraphics[width=0.5\linewidth]{./gfx/Chapter04/Corr3RepSLAM.pdf}}

\caption{Box-and-whisker plots depicting the errors obtained in two corridors, using either LSD-SLAM or appearance-based matching using SF-GABOR descriptors. The top row corresponds to the appearance-based result. The bottom row corresponds to LSD-SLAM. On each graph, the horizontal positions correspond to different journeys down the same corridor when the remainder of journeys is used as a database of journeys. Each of these positions represents the statistics of 100 random image queries. These graphs suggest that LSD-SLAM and an appearance-based approach are comparable in terms of reproducibility of localisation within the same corridor. Note, however, that much lower spatial resolution (less than 1/4 of the image size, in pixels) is used for the appearance-based technique than for LSD-SLAM.}
\label{fig:reprod}
\end{figure}

\subsubsection{Area-Under-Curve comparisons}
In Section \ref{visloc_perf} I calculated the average absolute positional error (in m) and the standard deviation of the absolute positional errors for a variety of methods. Here, I reproduce the AUC results for the method SF-GABOR, which ranged from 96.11\% to 96.39\%. For the case of LSD-SLAM, the AUC ranged from 89.71\% to 90.61\% All queries were again performed by adopting the leave-one-out strategy, but because of the high repeatability of results, we did not apply random frame-level sampling.  


\begin{figure}[h!]
\centering
%\setlength\figureheight{0.6\textwidth}
%\setlength\figurewidth{0.8\textwidth}
%\input{gfx/Chapter06/tikz/sfgabor_vs_lsdslam.tex} %
\includegraphics[width=0.8\textwidth]{gfx/Chapter04/SF_GABORvsLSD_SLAM.pdf}
\caption{CDF of SF-GABOR and LSD-SLAM when sampling 1000 random queries from all the error measurements. The width of the curves represent the variability of the result data. The bounds are the maximum and minimum CDF values obtained from the Monte-Carlo sampling.}
\label{fig:cdf}
\end{figure}



%------------------------------------------------------------------------- 
\section{A visualisation of the frame distribution with t-SNE}

In the previous sections I have extensively studied the case of the localisation within a journey, answering the question ``where am I along the path?'' that was introduced in Chapter \ref{ch:chapter2}. In a visual paths retrieval system divided in different journeys inside a building, to be able to answer the question ``in which path am I on?'' with precision would give this system the necessary prior information to provide a better location and also suggest path planning, which would be specially relevant in an assitive context as we will see in Chapter \ref{ch:chapter6}. Although the journey selection was beyond the scope of this thesis, it was informative to study the behaviour of a state-of-the-art dimensionality reduction technique in a rather challenging scenario of having such highly dimensional data (the BOVW-encoded vectors have 4,000 elements). Therefore I chose t-SNE as a technique for visualizing in two or three dimensions the high dimensional descriptor space of our RSM dataset.

t-distributed stochastic neighbor embedding (t-SNE) is a machine learning algorithm for dimensionality reduction developed by Laurens van der Maaten and Geoffrey Hinton \cite{maaten2009learning}. It is a nonlinear dimensionality reduction technique that is particularly well suited for embedding high-dimensional data into a space of two or three dimensions, which can then be visualised in a scatter plot. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modelled by nearby points and dissimilar objects are modelled by distant points.

In other words, t-SNE is a dimensionality reduction technique that aims to preserve the local structure of the data. For this reason I wanted to compute the visual path descriptions of the RSM dataset and build the foundations for future work on journey/corridor selection.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{gfx/Chapter04/tsne_dsift_3d.pdf}
\caption{Distribution of the BOW data of the RSM dataset in a reduced 3D space when visualised with t-SNE.}
\label{fig:tsne3d}
\end{figure}

In Figure \ref{fig:tsne3d} we can see the 4,000-dimension visual word reduced to three dimensions and in Figure \ref{fig:tsne2d} we can see the two-dimensional embedding. The embeddings were generated using more than 50,000 randomly selected examples from all the corridors. In this particular plot I selected dense-SIFT descriptors encoded with Hard-Assignment using k-means to create the visual word examples.

As we can see from the images, the difficulty of generating two or three-dimensional embeddings of such a high dimensional and complex dataset is notable. However, patterns showing how examples from the same corridors can display a sequential relationship within the embeddings.


\begin{figure}
\centering
\includegraphics[width=\textwidth]{gfx/Chapter04/tsne_dsift_2d.pdf}
\caption{Distribution of the BOW data of the RSM dataset in a reduced 2D space when visualised with t-SNE.}
\label{fig:tsne2d}
\end{figure}

The present thesis gives an emphasis on understanding \textit{visual path} data from a journey perspective, from a crowdsource collection of journeys in particular. However, although of limited practical use the within journey localisation, this visualisation is the first step in understanding the structure of the data from a ``building'' perspective.

It is therefore subject of future work the use of these visualisations to understand the important features of visual paths datasets such as the presence of global clusters that reveal remarkable distinctiveness between journeys or give insight on how to optimise the retrieval based on between-journey differences.

%------------------------------------------------------------------------- 
\section{Conclusion}

We have presented three main contributions to the topic of indoor localisation using visual path matching from wearable and hand-held cameras. In first place, we provide a complete evaluation of six local descriptor methods: three custom designed and three standard image (KP-SIFT and DSIFT) and video (HOG3D) matching methods as baseline. These local descriptions follow a standard bag-of-words and kernel encoding pipeline. The code for both the local descriptors and for the evaluation pipeline is available online~\cite{jose_rivera_rubio_2015_33762}. In second place, we also make available the RSM dataset a large dataset of more than 120,000 video-frames with positional ground truth of indoor journeys to complete the evaluation framework. The dataset is freely available at \url{http://rsm.bicv.org}. In third place, we introduce the CDFs of the error as the probability of the error being less than a certain threshold. This way of reporting localisation errors is a different performance metric than the usual average error with respect to the ground truth (although we also provide this) found in SLAM and other localisation approaches.

The results show that there is significant localisation information in the visual data, and that errors as small as \SI{1.5}{m} over a \SI{50}{m} distance can be achieved, even without tracking. As we mentioned above, the results have been reported in two ways: a) average absolute positional errors, and b) error distributions, both of which allow image descriptions to be assessed for their localisation capability.  The latter could also be used to build a measurement model for inclusion in a Kalman or particle filter  aimed at supporting human ambulatory navigation. 

As a baseline for these results, we provide a comparison with the state of the art method in SLAM, LSD-SLAM, with our best method, SF-GABOR. We found, rather surprisingly, that appearance-based localisation appeared to be at least as accurate as that of LSD-SLAM without loop closure over a distance of around 50 m. In particular, SF-GABOR presents a lower average localisation error ($\mu_{e,SF_GABOR} = 1.31 $ m $< \mu{e,LSD-SLAM} = 2.48 $ m. We also found that appearance-based localisation was achievable with low resolution images, of around 30,000 pixels per image -- half the number of pixels than in the frames used for the LSD-SLAM experiment. This lowers the computational burden, a potentially important factor in an assistive context where device power autonomy can hinder the use of power-hungry computer vision algorithms. In addition, the small file sizes required for our appearance-based localisation approach reduces the bandwidth and storage required for crowdsourcing data. We found that 1500-frame sequences, sufficient for a 50 m corridor at normal walking speeds, consumed no more than 2 MB once compressed, meaning that the journey segments required can feasibly be crowdsourced from several users within a building.

Finally, the high-dimensional visual word space representing the RSM dataset is visualised in 2D and 3D using t-SNE, a powerful dimensionality reduction algorithm for visualisations. The aim of this visualisation is to lay the foundations for future studies on retrieval at the building or large scale level, rather than at the journey level as in this thesis.

We plan to introduce tracking in future work. There are, of course, numerous other enhancements that one could make for a system that uses visual data; integration of data from other sensors springs to mind, such as inertial sensing, magnetometers and RSSI.  Although fusing independent and informative data sources would theoretically lead to improvements in performance, we would argue that the methods applied to infer location from each information source should be rigorously tested, both in isolation and as part of an integrated system.  This would help ensure that real-world systems would be somewhat robust to sensor failure. We anticipate that using vision to associate locations in the journeys of several users through their visual paths could play an important role in navigation. In fact, in the next chapter I delve deeper into the importance of appearance based retrieval techniques in localisation and explore the similarities of these techniques with those present in biology. I develop a model of biological mimicry to configure a localisation system inspired in biological place cells found in mammals.


\label{sec:conclusion}


%*****************************************
%*****************************************
%*****************************************
%*****************************************
%*****************************************
