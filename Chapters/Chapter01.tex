%************************************************
\chapter{Introduction}\label{ch:introduction}
%************************************************

In recent years, we have witnessed an unprecedented level of presence of technology in our lives. Mobile phones are smarter every day, with computational power that overtakes desktop computers just a couple of years old. At the same time, these devices gain in ubiquity as they extend their functionality to wearable technology, e.g. wearable cameras, wireless earphones or smart watches. The cameras installed in these devices have seen a similar increase in presence, resolution and quality of the lenses and sensors.

The combination of better cameras with improved processing and network connectivity opens the possibilities for computer vision to contribute to diverse applications, some of them having a special role in the inclusivity of people with disabilities and a positive impact in quality of life.

The diagram shown in Figure~\ref{fig:cv_dev_pipeline} illustrates a flow chart that depicts the process behind the development of an application that might depend on computer vision algorithms. In brief, this entails the development of a prototype that is later refined with the use of data. 

Data is the cornerstone for parameter optimisation and at the same time for applying the learning methods that are increasingly being made in many approaches that rely on artificial intelligence. Within computer vision, data, and in particular annotated data, has driven research very strongly. The availability of annotated datasets has achieved prominence, with the organisation of systematic and objective challenges around object recognition and detection; examples include PASCAL VOC \cite{everingham2010pascal} and ImageNet \citep{Deng2009}). Through these data collections and open competitions, benchmaarks for performance have been established that are now being extended to other fields within computer vision such as navigation (NAVVIS \cite{Huitl2012} and SLAMbench \cite{nardi2014introducing}.

In recent years, \textit{big data}, or the use of massive amounts of structured and unstructured data has made learning and prediction algorithms an important piece of the artificial intelligence puzzle. However, relying almost entirely in the use of data to solve the problem has sometimes caused the loss of perspective and very poor improvements on performance or none at all \cite{zhu2012we}. It is therefore to keep developing core algorithms and richer models that help solve artificial intelligence problems by gaining a better understanding of their constraints and representational structure. Often the solution is closer than what we think, and biology can provide us with efficient models that solve the problem in an effective way. In particular, in computer vision, models of the visual system have been shown to be effective in key tasks such as object recognition and visual localisation \cite{lowe2004distinctive,milford2004ratslam}. 


\begin{figure}
\centering
\includegraphics[width=\linewidth]{gfx/Chapter01/cv_dev_pipeline.pdf}
\caption{Stages in the development of a computer vision application. The development of an application in computer vision and similar artificial intelligence disciplines requires a different form of testing compared to the ones seen in standard software engineering toolchains. The use of databases and evaluation and optimisation stages prior to the development of the application complements the typical deterministic behaviour test suites.}
\label{fig:cv_dev_pipeline}
\end{figure}

\section{Mobile visual assistive apps}

Low vision brings many challenges to an individual, including reduced independence and social exclusion. The World Health Organization (WHO) estimates (2012) that more than 285 million people worldwide suffer from low vision or blindness. Due to changing demographics and greater incidence of disease -- e.g. diabetes -- blindness and failing sight are increasing in prevalence.  The cost to society includes direct health care expenditure, care-giver time and lost productivity.  Enabling people with visual impairment to increase participation will help address social exclusion and improve self-esteem and psychological well-being. There is the potential of near-commodity smartphones, backed by appropriate computer vision algorithms and supporting processes, to address this need.


The growth in availability of camera-equipped smartphones, networks, methods of social networking and crowdsourcing of data offers new solutions to develop assistive systems that could be scaled in performance and capability~\cite{Manduchi2012,Worsfold2010}. The services/capabilities that could be offered include:

\emph{Navigation}: GPS does not offer sufficient precision or reliability for indoor manoeuvring. A combination of visual cues, translated into speech or tactile information, is desirable.

\emph{Shopping}: Other challenges include shopping and product recognition, both in shops and at home. The technology for visual object recognition from mobile devices has arrived for sighted users: the challenges to deployment for visually-impaired users includes a) the existence of accessible label databases that are free from commercial bias; b) changing retrieval algorithms and systems to place more emphasis on strong match confidence; c) techniques for conveying information readily to blind and partially-sighted users.

\emph{Personal Safety}: As a partially sighted user, one is faced with a number of hurdles when undertaking journeys away from a familiar environment, and lack of confidence about the ``unseen'' can be a significant contributing factor to reduced mobility.  Where does the pavement end?  Where is the entrance to the bus, and are there stairs?  Are there obstructions at head-height?   

In summary, the overarching need is to increase the possibility for independent living; in a hugely visually-oriented built environment, sighted users rely on visual cues, signage, and recognition of structures such as doorways.  Can these cues be reliably translated into semantically appropriate information using computer vision? Therefore we focus on the feasibility of answering two questions with existing technology from visual cues: ``Where am I?'' and ``What am I holding?''. 
%% 

\section{Objectives}


\section{Organisation of the thesis}

The remainder of this dissertation is organised as follows: Chapter 2 presents our motivation on the acquisition of datasets and early work on appearance-based methods for visual localisation. Chapter 3 describes the SHORT dataset, a dataset for hand-held object recognition with an emphasis on the assistive context. Chapter 4 introduces our evaluation benchmark of descriptors for visual localisation. Chapter 5 presents our biologically inspired framework for visual localisation. In addition, we show experimental results of place-cells localisation  and a comparison with state-of-the-art SLAM. Chapter 6 describes our assistive vision-based localisation App that uses a wearable camera and a haptic feedback tablet to provide basic positional information. Finally in Chapter 7 we provide concluding remarks while summarising the main contributions of this dissertation and future work.


