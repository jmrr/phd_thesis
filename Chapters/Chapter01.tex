%************************************************
\chapter{Introduction}\label{ch:introduction}
%************************************************

In recent years, we have witnessed an unprecedented level of presence of technology in our lives. Mobile phones are smarter every day, with computational power that overtakes desktop computers just a couple of years old. At the same time, these devices gain in ubiquity as they extend their functionality to wearable technology, e.g. wearable cameras, wireless earphones or smart watches. The cameras installed in these devices have seen a similar increase in presence, resolution and quality of the lenses and sensors.

The combination of better cameras with improved processing and network connectivity opens the possibilities for computer vision to contribute to diverse applications, some of them having a special role in the inclusivity of people with disabilities and a positive impact in quality of life.

The diagram shown in Figure~\ref{fig:cv_dev_pipeline} illustrates a flow chart that depicts the process behind the development of an application that might depend on computer vision algorithms. In brief, this entails the development of a prototype that is later refined with the use of data. 

Data is the cornerstone for parameter optimisation and at the same time for applying the learning methods that are increasingly being made in many approaches that rely on artificial intelligence. Within computer vision, data, and in particular annotated data, has driven research very strongly. The availability of annotated datasets has achieved prominence, with the organisation of systematic and objective challenges around object recognition and detection; examples include PASCAL VOC \cite{everingham2010pascal} and ImageNet \citep{Deng2009}). Through these data collections and open competitions, benchmarks for performance have been established that are now being extended to other fields within computer vision such as localisation (NAVVIS \cite{Huitl2012} and SLAMbench \cite{nardi2014introducing}.

In recent years, \textit{big data}, or the use of massive amounts of structured and unstructured data has made learning and prediction algorithms an important piece of the artificial intelligence puzzle. However, relying almost entirely in the use of data to solve the problem has sometimes caused the loss of perspective and very poor improvements on performance or none at all \cite{zhu2012we}. It is therefore key to keep developing core algorithms and richer models that help solve artificial intelligence problems by gaining a better understanding of their constraints and representational structure. Often the solution is closer than what we think, and biology can provide us with efficient models that solve the problem in an effective way. In particular, in computer vision, models of the visual system have been shown to be effective in key tasks such as object recognition and visual localisation \cite{lowe2004distinctive,milford2004ratslam}. 


\begin{figure}
\centering
\includegraphics[width=\linewidth]{gfx/Chapter01/cv_dev_pipeline.pdf}
\caption{Stages in the development of a computer vision application. The development of an application in computer vision and similar artificial intelligence disciplines requires a different form of testing compared to the ones seen in standard software engineering toolchains. The use of databases and evaluation and optimisation stages prior to the development of the application complements the typical deterministic behaviour test suites.}
\label{fig:cv_dev_pipeline}
\end{figure}

\section{Mobile visual assistive apps}

Low vision brings many challenges to an individual, including reduced independence and social exclusion. The World Health Organization (WHO) estimates (2012) that more than 285 million people worldwide suffer from low vision or blindness. Due to changing demographics and greater incidence of disease -- e.g. diabetes -- blindness and failing sight are increasing in prevalence.  The cost to society includes direct health care expenditure, care-giver time and lost productivity.  Enabling people with visual impairment to increase participation will help address social exclusion and improve self-esteem and psychological well-being. There is the potential of near-commodity smartphones, backed by appropriate computer vision algorithms and supporting processes, to address this need.

The growth in availability of camera-equipped smartphones, networks, methods of social networking and crowdsourcing of data offers new solutions to develop assistive systems that could be scaled in performance and capability~\cite{Manduchi2012,Worsfold2010}. The services/capabilities that could be offered include:

\emph{Navigation}: Regardless of whether one is outdoors or indoors, navigation in sighted humans relies heavily on the sense of vision~\citep{kalia2008learning,tsuji2005landmarks}. When vision is deteriorated or deprived, a person's ability to navigate -- particularly in unfamiliar settings -- is greatly diminished. Indeed, a significant proportion of individuals who experience sight loss late in life find navigation in unfamiliar environments challenging. This might be a key reason contributing to the fact that more than half (55\%) of the blind and partially sighted in the UK rarely venture outside of their homes \citep{Worsfold2010}. Additionally, GPS does not offer sufficient precision or reliability for indoor manoeuvring. A combination of visual cues, translated into speech or tactile information, is desirable.

\emph{Shopping}: Other challenges include shopping and product recognition, both in shops and at home. The technology for visual object recognition from mobile devices has arrived for sighted users: the challenges to deployment for visually-impaired users includes a) the existence of accessible label databases that are free from commercial bias; b) changing retrieval algorithms and systems to place more emphasis on strong match confidence; c) techniques for conveying information readily to blind and partially-sighted users.

\emph{Personal Safety}: As a partially sighted user, one is faced with a number of hurdles when undertaking journeys away from a familiar environment, and lack of confidence about the ``unseen'' can be a significant contributing factor to reduced mobility.  Where does the pavement end?  Where is the entrance to the bus, and are there stairs?  Are there obstructions at head-height?   

In summary, the overarching need is to increase the possibility for independent living; in a hugely visually-oriented built environment, sighted users rely on visual cues, signage, and recognition of structures such as doorways.  Can these cues be reliably translated into semantically appropriate information using computer vision? Therefore we focus on the feasibility of answering two questions with existing technology from visual cues: ``Where am I?'' and ``What am I holding?''. 
%% 


\section{Appearance-based methods}

In computer vision, image representation methods can be divided in three approaches. Model-based approaches attempt to represent images as a combination of different geometrical shapes, i.e. boxes and circles~\cite{biederman1987}. In contour-based approaches image representations are defined by the edges of the structures present in them~\cite{canny1986computational}. The last approach and subject of this thesis aims to represent visual information by its \textit{appearance}. 

The appearance of an object or a scene in an image can be represented by a series of views from different distances and angles, although usually humans only need a few to recognise more than 30,000 different objects~\cite{biederman1987recognition} solely by their appearance, even when this is partially occluded. 

The set of image representations generated by appearance-based methods i commonly referred as image \textit{features} or \textit{descriptors} since they \textit{describe} properties of the image. Features can be sub-divided into two groups depending on whether they focus on a point or a small area within the images (local approaches) or whether they represent each image in its entirety (global approaches), taking into account all pixels in the image. Good feature traits are robustness against noise, rotation and scale or illumination changes. 

In this thesis, the interest is on local region features. The process to compute these region features is comprised of two steps. In the first one, features that represent one point in the image due to some particular properties of those points are calculated. These are called \textit{interesting points} or \textit{keypoints}. Although keypoints represent interesting points in the image, they can also convey information about the scale and orientation to characterise properties of their local region such as stability. Some of the most widely used keypoint detectors are the Harris corner detector~\cite{harris1988combined}, Difference of Gaussian (DoG) detector or Scale-invariant Feature Transform (SIFT) keypoint detector~\cite{lowe2004}; and Maximally Stable Extremal Regions (MSER) detector~\cite{matas2004robust}.

Once the keypoints have been detected for an image, the next step in order to compute the region features is to perform some image processing algorithms on a patch surrounding the keypoint. The output of this computation is normally stored in a vector, the feature vector or \textit{feature descriptor}. The descriptors include a considerable amount of information about the region and in this thesis we will show that the algorithms to compute them 	are suitable for different tasks.

There is an increasing number of feature description algorithms, with most of the research focusing on the invariability of the descriptors with image transformations (rotation, scale, deformations among others). However, two of them are particularly well known, Speeded Up Robust Features (SURF) and especially SIFT, nowadays widely and increasingly extended since its main drawback, the computational load (due to the high dimensionality of its descriptor) is being overcome by the recent advances in multi-core CPUs and particularly graphic processing units (GPU) \cite{Wu2007}. Therefore, it has been used as a baseline method for performance evaluation throughout this thesis.


The image representations achieved by appearance-based approaches can be used for a multitude of artificial intelligence tasks: object recognition, visual localisation, scene detection, face recognition, human identification and action recognition to name only a few. As mentioned in the previous section, this thesis will cover the first two applications, object recognition and visual localisation, with emphasis in the challenging scenarios of hand-held image acquisition and assistive applications. My hypothesis in this aspect is that appearance-based methods can be, in isolation, valid solutions for many challenging problems simultaneously, and combined with other sensing (e.g. WiFi, inertial and depth sensors) and inference techniques (e.g. tracking) it can be a complementary technology that help improve the overall performance of the different approaches on their own. Specifically, as a means to test this hypothesis, it is also my goal to evaluate which appearance-based techniques adapt better to the needs of hand-held object recognition and visual navigation in terms of accuracy, power, time and computational load and assess their room for performance.

\section{Biological evidence of place cell visual localisation} 

Place cell behavior is usually found by obtaining electrical recordings from several biological neurons (CA1) in specific regions of the hippocampus; by comparing firing rates between neurons and relative to background firing rates, one can infer which neurons are displaying place-cell behaviour. The concept of a biological place cell (BPC) is illustrated in Figure~\ref{fig:Maze}.  The coloured circles depict locations within a maze in which individual place cells show elevated firing relative to other place cells.  Through observing this electrical behaviour over many trials, firing patterns can be decoded, and used as an indication of where the rat is along the maze: in the simplest case, the BPC which fires maximally is taken as the rat's approximate location in space. \\

\begin{figure}[!t]
\centering
\includegraphics[width=.6\linewidth]{./gfx/Chapter01/bio_place_cells_rat.pdf}
\caption{Illustration of place cells; this figure is from experiments of S.P. Layton in behaving rats. Small coloured circles depict individual biological place-cells having maximum activation (firing rate) compared to all other place cells throughout the navigation space. Licensed for use under the Creative Commons Attribution-ShareAlike 3.0 License.}
\label{fig:Maze}
\end{figure}

There is no suggestion that the inference in place cells is purely a binary form of ``you are here'' type of response.  Instead, it is likely that the firing rates of several place cells are combined to more accurately infer position \cite{hafting2005microstructure}, a process that might involve both phase-locked patterns of firing and rate codes \cite{dragoi2006temporal}. Place cell selectivity itself is thought to arise from a combination of inputs and cell responses inside and outside hippocampal areas. Rate coding \cite{van2001rate} is just one mechanism by which groups of neurons are thought to encode and represent sensory information. A rate code can be interpreted by estimating average firing rates (over trials) within the same neuron. %Examples of average firing rates from individual place cells are illustrated in a rat moving along a one-dimensional (linear track) corridor in Fig.~\ref{fig:RateCode}.\\
The visual information captured by the eyes of an animal should be seen as only one of the many sensory and internal cues that lead to the spatially selective nature of biological place-cell responses \cite{hassabis2009decoding}. Nevertheless, in many animals, and certainly in humans and primates, vision is a particularly strong cue as to one's position \cite{epstein1998cortical}, therefore another research hypothesis for this thesis is whether place cells can be modelled with appearance-based methods and be used for inferring locations by only using visual input.


%\section{Objectives}

%The main goal of this work is to investigate, formulate, implement and evaluate appearance-based methods that might provide a better representation of the visual inputs produced in the contexts of hand-held object recognition and visual localisation. To this end, six principal objectives have been set


%\begin{enumerate}
%\item First, I analysed the impact of computer vision in mobile and wearable technologies in an assistive context, providing studies of appearance-based methods for two applications, hand-held object recognition of household products and indoor navigation.
%
%\item \textbf{Artificial Place Cell Model} Second, I have provided a novel artificial place cell (APC) model that appear to replicate rate-coding behaviour of their biological counterparts found in the hippocampus. These models were tested under challenging conditions of indoor navigation by using a generalised regression neural network as a training mechanism for learning a positional ground truth from a database.
%
%\item \textbf{Prototype of an assistive application} I took these previous findings to the next step and developed a prototype client-server Android application for assistive localisation from wearable and hand-held devices using their visual input and a haptic feedback tablet (the Senseg\texttrademark) to provide tactile cues to the location estimates. With this work I laid out the foundations for a crowdsourcing approach that extens the idea of using sensor data from wearable devices to localise a person.
%
%\item \textbf{Two novel datasets} These contributions are accompanied by two datasets, namely the SHORT dataset for hand-held object recognition and the RSM dataset of \emph{visual paths}.
%\end{enumerate}

%% ch02

%\hspace{0.5cm}First, we suggest how to go about providing estimates of the indoor location of a user through queries submitted by a smartphone camera against a database of {\em visual paths} -- descriptions of the visual appearance of common journeys that might be taken. Our proposal is that such journeys could be harvested from, for example, sighted volunteers. Initial tests using bootstrap statistics do indeed suggest that there is sufficient information within such visual path data to provide indications of: a) along which of several routes a user might be navigating; b) where along a particular path they might be.
%
%\hspace{0.5cm} We will also discuss a pilot benchmarking database and test set for answering the second question of ``What am I holding?''.  We evaluated the role of video sequences, rather than individual images, in such a query context, and suggest how the extra information provided by temporal structure could significantly improve the reliability of search results, an important consideration for assistive applications.
%

%% ch03

%Visual object recognition is just one of the many applications of camera-equippe
%d smartphones. The ability to recognise objects through photos taken with wearab
%le and handheld cameras is already possible through some of the larger internet 
%search providers; yet, there is little rigorous analysis of the quality of searc
%h results, particularly where there is great disparity in image quality. This ha
%s motivated us to develop the Small Hand-held Object Recognition Test (SHORT). T
%his includes a dataset that is suitable for recognising hand-held objects from e
%ither snapshots or videos acquired using hand-held or wearable cameras. SHORT pr
%ovides a collection of images and ground truth that help evaluate the different 
%factors that affect recognition performance. At its present state, the dataset i
%s comprised of a set of high quality training images and a large set of nearly 1
%35,000 smartphone-captured test images of 30 grocery products. In this paper, we
% will discuss some open challenges in the visual object recognition of objects t
%hat are being held by users. We evaluate the performance of a number of popular
%object recognition algorithms, with differing levels of complexity, when tested
%against SHORT.

%% ch04

%In this paper, we address a specific use-case of wearable or handheld camera technology related to indoor navigation. The main question we address is whether it is possible to crowdsou
%rce navigational data in the form of video sequences captured from wearable cameras.  Without using geometric inference techniques (such as SLAM), we test video data for navigational c
%ontent, and algorithms for extracting that content.  We do not include tracking in this evaluation: our purpose is to explore the hypothesis that visual content, on its own, contains c
%ues that can be mined to infer a person's location.  We test this hypothesis through estimating the positional error inferred during one journey with respect to other journeys along th
%e same approximate path.
%
%The contribution of this paper is threefold. First, we propose alternative methods for video feature extraction that identifies candidate matches between query sequences against a data
%base of previously acquired sequences. Secondly, we propose an evaluation methodology that estimates the error distributions in position inference with respect to the ground truth. In
%the evaluation we compare standard approaches in the retrieval context, such as SIFT and HOG3D, to establish positional estimates. The final contribution is a publicly available databa
%se comprising over 90,000 frames of video-sequences with positional ground-truth in the form of position along a path. The data was acquired along more than 3 km worth of indoor journe
%ys with a hand-held device (Nexus 4) and a wearable device (Google Glass).

%Experimental results show that image queries against previously acquired visual paths could contribute to positional estimates used in navigation. The evaluation also yields that sing
%le-frame query methods work better than spatio-temporal queries in the context of these tests which do not use explicit tracking or self-motion estimation.



%% ch05

%Animals use a variety of environmental cues in order to recognise their location
%.  One of the key behaviours found in a certain type of biological neuron -- kno
%wn as place cells -- is a rate-coding effect: a neuron's rate of firing decrease
%s with distance from some landmark location. In this work, we used visual inform
%ation from wearable and hand-held cameras in order to reproduce this rate-coding
% effect in artificial place cells (APCs).  The accuracy of localisation using th
%ese APCs was evaluated using different visual descriptors and different place ce
%ll widths. Simple localisation using APCs was feasible by noting the identity of
% the APC yielding the maximum response. We also propose using joint coding withi
%n a number of automatically defined APCs as a population code for self-localisat
%ion. Using both approaches we were able to demonstrate good self-localisation fr
%om very small images taken in indoor settings. The error performance using APCs
%is favourable when compared with ground-truth and LSD-SLAM, even without the use
% of a motion model.


\section{Objectives and thesis layout}

The main goal of this work is to investigate, formulate, implement and evaluate appearance-based methods that might provide a better representation of the visual inputs produced in the contexts of hand-held object recognition and visual localisation. 

The remainder of this dissertation is organised as follows: Chapter 2 presents our motivation on the acquisition of datasets and early work on appearance-based methods for visual localisation. Chapter 3 describes the SHORT dataset, a dataset for hand-held object recognition with an emphasis on the assistive context. Chapter 4 introduces our evaluation benchmark of descriptors for visual localisation. Chapter 5 presents our biologically inspired framework for visual localisation. In addition, we show experimental results of place-cells localisation  and a comparison with state-of-the-art SLAM. Chapter 6 describes our assistive vision-based localisation App that uses a wearable camera and a haptic feedback tablet to provide basic positional information. Finally in Chapter 7 we provide concluding remarks while summarising the main contributions of this dissertation and future work.

\td{Change?}{Perhaps use brief abstracts here}{This dissertation is organized in seven chapters, in which the first chapter has covered the motivation, state-of-the-art and objectives. Next, an outline
of contents of the rest of the chapters are provided}

